{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 : HW2 Report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Input Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I read the data from \"tweets_DM.json\"  and merge tweet_id from \"data_identification.csv\" and \"tweets_DM.json\".\n",
    "\n",
    "Then, concatenated the dataframe with the emotion label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After processing all the input data, the next thing is to process the text data.\n",
    "\n",
    "1. Delete \"LH\" token in the text\n",
    "2. Convert all text into lowercase\n",
    "3. Replace @user token with \"\" token.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. \n",
    "### Method 1 : Train naive bayes classifier as baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In method 1, I create a baseline with naive bayes classifier, with only utilize top 50k unigrams and bigrams BOW features. The f1 score of naive bayes classifier is 0.44 on validation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Snapshot/snapshot5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Snapshot/snapshot1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the public test data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Snapshot/snapshot2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 : Using BiLSTM_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Snapshot/snapshot4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Snapshot/snapshot1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3 : Train BERT model with training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 : Load pre-train word embeddings model\n",
    "In this method, I used pre-trained BERT-based word embedding vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 : Input data preparation\n",
    "\n",
    "1. Split the data into training set and testing set (test_size=0.2).\n",
    "\n",
    "2. Use keras Tokenizer to tokenize text.\n",
    "\n",
    "3. Use keras pad_sequences to pad the text (maxlen=50).\n",
    "\n",
    "4. Utilize keras.utils.to_categorical for emotion label.(8 categoricals total)\n",
    "\n",
    "5. Create word embedding matrix with pre-trained embeddings.(BERT embedding)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 : Build BERT model and train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model that I designed consists the following layers below:\n",
    "\n",
    "1. Word embeddings layer (not trainable).\n",
    "2. Two Drop layer.\n",
    "3. Three Dense layer.\n",
    "\n",
    "![title](Snapshot/snapshot6.jpg)\n",
    "   \n",
    "I used categorical cross entropy as loss function. As for metrics, since keras has removed f1-score from Metrics since 2.0      version, I needed to implement it with keras Callback. Also, the model trained for 3 epochs with batch size 32 will be the highest accuracy in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework, I focus on doing the data preprocessing and utilize different kind of methods.\n",
    "And I find out that there will have GPU memory problem when your model is too big, so I utilize gpu setting prevent from memory\n",
    "explosion.\n",
    "\n",
    "There is another thing that I try is to increase the cover rate by replacing some tokens that doesn't appears in the dictionary, but after running through all the dataset for training in BERT method, the accuracy won't be higher than we didn't do the data preprocessing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Future Plan:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble other features with current BERT model:\n",
    "1. Score\n",
    "2. Date\n",
    "3. Hashtags\n",
    "4. Do some data cleaning\n",
    "   \n",
    "Of course, look closer to the data in the dataset.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
