{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Information\n",
    "Name:張楚翎/Lilian\n",
    "\n",
    "Student ID: 109065508\n",
    "\n",
    "GitHub ID: lilian5319229\n",
    "\n",
    "Kaggle name: Lilian Chang\n",
    "\n",
    "Kaggle private scoreboard snapshot:\n",
    "\n",
    "[Snapshot](img/pic0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First: __This part is worth 30% of your grade.__ Do the **take home** exercises in the [DM2020-Lab2-Master Repo](https://github.com/fhcalderon87/DM2020-Lab2-Master). You may need to copy some cells from the Lab notebook to this notebook. \n",
    "\n",
    "\n",
    "2. Second: __This part is worth 30% of your grade.__ Participate in the in-class [Kaggle Competition](https://www.kaggle.com/c/dm2020-hw2-nthu/) regarding Emotion Recognition on Twitter. The scoring will be given according to your place in the Private Leaderboard ranking: \n",
    "    - **Bottom 40%**: Get 20% of the score (ie. 20% of 30% )\n",
    "\n",
    "    - **Top 41% - 100%**: Get (101-x)% of the score, where x is your ranking in the leaderboard (ie. (101-x)% of 30% )   \n",
    "    Submit your last submission __BEFORE the deadline (Dec. 5th 11:59 pm, Saturday)__. Make sure to take a screenshot of your position at the end of the competition and store it as '''pic0.png''' under the **img** folder of this repository and rerun the cell **Student Information**.\n",
    "    \n",
    "\n",
    "3. Third: __This part is worth 30% of your grade.__ A report of your work developping the model for the competition (You can use code and comment it). This report should include what your preprocessing steps, the feature engineering steps and an explanation of your model. You can also mention different things you tried and insights you gained. \n",
    "\n",
    "\n",
    "4. Fourth: __This part is worth 10% of your grade.__ It's hard for us to follow if your code is messy :'(, so please **tidy up your notebook** and **add minimal comments where needed**.\n",
    "\n",
    "\n",
    "You can submit your homework following these guidelines: [Git Intro & How to hand your homework](https://github.com/fhcalderon87/DM2020-Lab1-Master/blob/master/Git%20Intro%20%26%20How%20to%20hand%20your%20homework.ipynb), but make sure to fork the [DM2020-Lab2-Homework](https://github.com/fhcalderon87/DM2020-Lab2-Homework) repository this time! Also please __DON´T UPLOAD HUGE DOCUMENTS__, please use Git ignore for that.\n",
    "\n",
    "Make sure to commit and save your changes to your repository __BEFORE the deadline (Dec. 8th 11:59 pm, Tuesday)__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before beggining the lab, please make sure to download the [Google News Dataset](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit) and place it in a folder named \"GoogleNews\" in the same directory as this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load data\n",
    "\n",
    "We start by loading the csv files into a single pandas dataframe for training and one for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "### training data\n",
    "anger_train = pd.read_csv(\"data/semeval/train/anger-ratings-0to1.train.txt\",\n",
    "                         sep=\"\\t\", header=None,names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "sadness_train = pd.read_csv(\"data/semeval/train/sadness-ratings-0to1.train.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "fear_train = pd.read_csv(\"data/semeval/train/fear-ratings-0to1.train.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "joy_train = pd.read_csv(\"data/semeval/train/joy-ratings-0to1.train.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine 4 sub-dataset\n",
    "train_df = pd.concat([anger_train, fear_train, joy_train, sadness_train], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000</td>\n",
       "      <td>How the fu*k! Who the heck! moved my fridge!.....</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10001</td>\n",
       "      <td>So my Indian Uber driver just called someone t...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10002</td>\n",
       "      <td>@DPD_UK I asked for my parcel to be delivered ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10003</td>\n",
       "      <td>so ef whichever butt wipe pulled the fire alar...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10004</td>\n",
       "      <td>Don't join @BTCare they put the phone down on ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text emotion  intensity\n",
       "0  10000  How the fu*k! Who the heck! moved my fridge!.....   anger      0.938\n",
       "1  10001  So my Indian Uber driver just called someone t...   anger      0.896\n",
       "2  10002  @DPD_UK I asked for my parcel to be delivered ...   anger      0.896\n",
       "3  10003  so ef whichever butt wipe pulled the fire alar...   anger      0.896\n",
       "4  10004  Don't join @BTCare they put the phone down on ...   anger      0.896"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### testing data\n",
    "anger_test = pd.read_csv(\"data/semeval/dev/anger-ratings-0to1.dev.gold.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "sadness_test = pd.read_csv(\"data/semeval/dev/sadness-ratings-0to1.dev.gold.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "fear_test = pd.read_csv(\"data/semeval/dev/fear-ratings-0to1.dev.gold.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "joy_test = pd.read_csv(\"data/semeval/dev/joy-ratings-0to1.dev.gold.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "\n",
    "# combine 4 sub-dataset\n",
    "test_df = pd.concat([anger_test, fear_test, joy_test, sadness_test], ignore_index=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle dataset\n",
    "train_df = train_df.sample(frac=1)\n",
    "test_df = test_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training df:  (3613, 4)\n",
      "Shape of Testing df:  (347, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Training df: \", train_df.shape)\n",
    "print(\"Shape of Testing df: \", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 1 (Take home): **  \n",
    "Plot word frequency for Top 30 words in both train and test dataset. (Hint: refer to DM lab 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 ... 2 2 1]\n",
      "8930\n",
      "9119\n",
      "537\n",
      "10051\n",
      "6371\n",
      "4546\n",
      "4571\n",
      "4383\n",
      "8924\n",
      "5957\n",
      "3391\n",
      "6422\n",
      "976\n",
      "5577\n",
      "9020\n",
      "8264\n",
      "4009\n",
      "6235\n",
      "4798\n",
      "1448\n",
      "9872\n",
      "757\n",
      "1504\n",
      "5186\n",
      "464\n",
      "670\n",
      "10059\n",
      "9678\n",
      "9782\n",
      "4319\n",
      "['the', 'to', 'and', 'you', 'of', 'is', 'it', 'in', 'that', 'my', 'for', 'on', 'be', 'me', 'this', 'so', 'have', 'not', 'just', 'but', 'with', 'at', 'can', 'like', 'all', 'are', 'your', 'was', 'when', 'if']\n",
      "[1736 1289  955  796  793  787  717  626  537  535  446  390  383  380\n",
      "  350  328  312  303  301  293  288  269  257  243  243  231  229  228\n",
      "  226  216]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_vect_train = CountVectorizer()\n",
    "X_counts_train = count_vect_train.fit_transform(train_df.text)\n",
    "term_frequencies_train = X_counts_train.toarray().sum(axis=0)\n",
    "#check the term frequencies\n",
    "print(term_frequencies_train)\n",
    "N = 30\n",
    "index = np.arange(N)\n",
    "\n",
    "#store the top 30 words in top_term_train\n",
    "top_term_train = []\n",
    "for i in np.argsort(term_frequencies_train)[::-1][:N]:\n",
    "    print(i)\n",
    "    top_term_train.append(count_vect_train.get_feature_names()[i])\n",
    "    \n",
    "y_train = np.sort(term_frequencies_train)[::-1][:N]\n",
    "print(top_term_train)\n",
    "print(y_train)\n",
    "\n",
    "#plot the figure\n",
    "fig = plt.figure(figsize=(20, 5))\n",
    "plt.title('Top 30 term in Train dataset')\n",
    "fig = plt.bar(top_term_train,y_train, 0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 2 ... 1 1 1]\n",
      "1687\n",
      "1739\n",
      "99\n",
      "909\n",
      "1240\n",
      "1938\n",
      "903\n",
      "867\n",
      "655\n",
      "1686\n",
      "1165\n",
      "1257\n",
      "185\n",
      "1088\n",
      "1843\n",
      "1567\n",
      "290\n",
      "150\n",
      "785\n",
      "82\n",
      "934\n",
      "133\n",
      "1893\n",
      "277\n",
      "705\n",
      "1940\n",
      "1804\n",
      "1712\n",
      "1009\n",
      "1212\n",
      "['the', 'to', 'and', 'it', 'of', 'you', 'is', 'in', 'for', 'that', 'my', 'on', 'be', 'me', 'was', 'so', 'can', 'at', 'have', 'all', 'just', 'are', 'with', 'but', 'get', 'your', 'up', 'this', 'like', 'not']\n",
      "[153 138  92  83  83  82  77  69  57  55  52  46  40  36  34  34  32  30\n",
      "  30  30  29  29  29  26  26  24  24  24  24  24]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_vect_test = CountVectorizer()\n",
    "X_counts_test = count_vect_test.fit_transform(test_df.text)\n",
    "term_frequencies_test = X_counts_test.toarray().sum(axis=0)\n",
    "print(term_frequencies_test)\n",
    "N = 30\n",
    "index = np.arange(N)\n",
    "\n",
    "top_term_test = []\n",
    "for i in np.argsort(term_frequencies_test)[::-1][:N]:\n",
    "    print(i)\n",
    "    top_term_test.append(count_vect_test.get_feature_names()[i])\n",
    "    \n",
    "y_test = np.sort(term_frequencies_test)[::-1][:N]\n",
    "print(top_term_test)\n",
    "print(y_test)\n",
    "\n",
    "#plot the figure\n",
    "fig = plt.figure(figsize=(20, 5))\n",
    "plt.title('Top 30 term in Test dataset')\n",
    "fig = plt.bar(top_term_test,y_test, 0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.2 Save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save our data in Pickle format. The pickle module implements binary protocols for serializing and de-serializing a Python object structure.   \n",
    "  \n",
    "Some advantages for using pickle structure:  \n",
    "* Because it stores the attribute type, it's more convenient for cross-platform use.  \n",
    "* When your data is huge, it could use less space to store also consume less loading time.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save to pickle file\n",
    "train_df.to_pickle(\"train_df.pkl\") \n",
    "test_df.to_pickle(\"test_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## load a pickle file\n",
    "train_df = pd.read_pickle(\"train_df.pkl\")\n",
    "test_df = pd.read_pickle(\"test_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information: https://reurl.cc/0Dzqx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature engineering\n",
    "### Using Bag of Words\n",
    "Using scikit-learn ```CountVectorizer``` perform word frequency and use these as features to train a model.  \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:484: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3613, 500)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# build analyzers (bag-of-words)\n",
    "BOW_500 = CountVectorizer(max_features=500, tokenizer=nltk.word_tokenize) \n",
    "\n",
    "# apply analyzer to training data\n",
    "BOW_500.fit(train_df['text'])\n",
    "\n",
    "train_data_BOW_features_500 = BOW_500.transform(train_df['text'])\n",
    "\n",
    "## check dimension\n",
    "train_data_BOW_features_500.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 2, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_BOW_features_500.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['change',\n",
       " 'cheer',\n",
       " 'cheerful',\n",
       " 'cheering',\n",
       " 'cheery',\n",
       " 'class',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'could',\n",
       " 'country']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe some feature names\n",
    "feature_names_500 = BOW_500.get_feature_names()\n",
    "feature_names_500[100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 2 (Take home): **  \n",
    "Generate an embedding using the TF-IDF vectorizer instead of th BOW one with 1000 features and show the feature names for features [100:110]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3613, 500)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Answer here\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "TFIDF_500 = TfidfVectorizer(max_features=500, tokenizer=nltk.word_tokenize)\n",
    "\n",
    "TFIDF_500.fit(train_df['text'])\n",
    "train_data_TFIDF_500 = TFIDF_500.transform(train_df['text'])\n",
    "\n",
    "train_data_TFIDF_500.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['change',\n",
       " 'cheer',\n",
       " 'cheerful',\n",
       " 'cheering',\n",
       " 'cheery',\n",
       " 'class',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'could',\n",
       " 'country']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names_TFIDF_500=TFIDF_500.get_feature_names()\n",
    "feature_names_TFIDF_500[100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Model\n",
    "### 3.1 Decision Trees\n",
    "Using scikit-learn ```DecisionTreeClassifier``` performs word frequency and uses these as features to train a model.  \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# for a classificaiton problem, you need to provide both training & testing data\n",
    "X_train = BOW_500.transform(train_df['text'])\n",
    "y_train = train_df['emotion']\n",
    "\n",
    "X_test = BOW_500.transform(test_df['text'])\n",
    "y_test = test_df['emotion']\n",
    "\n",
    "## take a look at data dimension is a good habbit  :)\n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build DecisionTree model\n",
    "DT_model = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "## training!\n",
    "DT_model = DT_model.fit(X_train, y_train)\n",
    "\n",
    "## predict!\n",
    "y_train_pred = DT_model.predict(X_train)\n",
    "y_test_pred = DT_model.predict(X_test)\n",
    "\n",
    "## so we get the pred result\n",
    "y_test_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 3 (Take home): **  \n",
    "Can you interpret the results above? What do they mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer here\n",
    "Because the Decision Tree classifier has issues of underfitting and overfitting. In the result, we can see that there is a huge accuracy gap between training data and testing data, so we can clearly find out that there is an overfitting problem in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 4 (Take home): **  \n",
    "Build a model using a ```Naive Bayes``` model and train it. What are the testing results? \n",
    "\n",
    "*Reference*: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and train model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "NB_model = MultinomialNB()\n",
    "NB_model = NB_model.fit(X_train, y_train)\n",
    "\n",
    "# get predicted result on test data\n",
    "y_train_pred = NB_model.predict(X_train)\n",
    "y_test_pred = NB_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc_train = accuracy_score(y_true=y_train, y_pred=y_train_pred)\n",
    "acc_test = accuracy_score(y_true=y_test, y_pred=y_test_pred)\n",
    "\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))\n",
    "print('testing accuracy: {}'.format(round(acc_test, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## precision, recall, f1-score,\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check by confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(confusion_matrix(y_true=y_test, y_pred=y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 5 (Take home): **  \n",
    "\n",
    "How do the results from the Naive Bayes model and the Decision Tree model compare? How do you interpret these differences? Use the theoretical background covered in class to try and explain these differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training accuracy: DT > NB \n",
    "\n",
    "Testing accuracy: NB > DT\n",
    "\n",
    "\n",
    "We can follow the result in Exercise 3, the Decision Tree model has the issue of overfitting. \n",
    "Although the Navie Bayes model get lower accuracy than Decision Tree model in train dataset, it acctually get better result in testing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Deep Learning\n",
    "\n",
    "We use [Keras](https://keras.io/) to be our deep learning framwork, and follow the [Model (functional API)](https://keras.io/models/model/) to build a Deep Neural Network (DNN) model. Keras runs with Tensorflow in the backend. It's a nice abstraction to start working with NN models. \n",
    "\n",
    "Because Deep Learning is a 1-semester course, we can't talk about each detail about it in the lab session. Here, we only provide a simple template about how to build & run a DL model successfully. You can follow this template to design your model.\n",
    "\n",
    "We will begin by building a fully connected network, which looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fully Connected Network](pics/pic1.png)\n",
    "\n",
    "(source: https://github.com/drewnoff/spark-notebook-ml-labs/tree/master/labs/DLFramework)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Prepare data (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "# standardize name (X, y) \n",
    "X_train = BOW_500.transform(train_df['text'])\n",
    "y_train = train_df['emotion']\n",
    "\n",
    "X_test = BOW_500.transform(test_df['text'])\n",
    "y_test = test_df['emotion']\n",
    "\n",
    "## check dimension is a good habbit \n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Deal with categorical label (y)\n",
    "\n",
    "Rather than put your label `train_df['emotion']` directly into a model, we have to process these categorical (or say nominal) label by ourselves. \n",
    "\n",
    "Here, we use the basic method [one-hot encoding](https://en.wikipedia.org/wiki/One-hot) to transform our categorical  labels to numerical ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## deal with label (string -> one-hot)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = label_encode(label_encoder, y_train)\n",
    "y_test = label_encode(label_encoder, y_test)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I/O check\n",
    "input_shape = X_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](pics/pic2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  tensorflow.keras.models import Model\n",
    "from  tensorflow.keras.layers import Input, Dense\n",
    "from  tensorflow.keras.layers import ReLU, Softmax\n",
    "\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 500\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=64)(X)  # 64\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=64)(H1)  # 64\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# output layer\n",
    "H2_W3 = Dense(units=output_shape)(H2)  # 4\n",
    "H3 = Softmax()(H2_W3)\n",
    "\n",
    "model_output = H3\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('logs/training_log.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 25\n",
    "batch_size = 32\n",
    "\n",
    "# training!\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger],\n",
    "                    validation_data = (X_test, y_test))\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Predict on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict\n",
    "pred_result = model.predict(X_test, batch_size=128)\n",
    "pred_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result = label_decode(label_encoder, pred_result)\n",
    "pred_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('testing accuracy: {}'.format(round(accuracy_score(label_decode(label_encoder, y_test), pred_result), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's take a look at the training log\n",
    "training_log = pd.DataFrame()\n",
    "training_log = pd.read_csv(\"logs/training_log.csv\")\n",
    "training_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 6 (Take home): **  \n",
    "\n",
    "Plot the Training and Validation Accuracy and Loss (different plots), just like the images below (Note: the pictures below are an example from a different model). How to interpret the graphs you got? How are they related to the concept of overfitting/underfitting covered in class?\n",
    "<table><tr>\n",
    "    <td><img src=\"pics/pic3.png\" style=\"width: 300px;\"/> </td>\n",
    "    <td><img src=\"pics/pic4.png\" style=\"width: 300px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1,2,1)\n",
    "y_acc=training_log.loc[:,'accuracy']\n",
    "y_val_acc=training_log.loc[:,'val_accuracy']\n",
    "plt.plot(range(len(y_acc)),y_acc,label=\"Train accuracy\",color=\"b\")\n",
    "plt.plot(range(len(y_acc)),y_val_acc,label=\"Val accuracy\",color=\"r\")\n",
    "plt.title('Training Accuracy per epoch')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "y_loss=training_log.loc[:,'loss']\n",
    "y_val_loss=training_log.loc[:,'val_loss']\n",
    "plt.plot(range(len(y_loss)),y_loss,label=\"Train loss\",color=\"b\")\n",
    "plt.plot(range(len(y_val_loss)),y_val_loss,label=\"Val loss\",color=\"r\")\n",
    "plt.title('Training Loss per epoch')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "During the training process, the model tries to minimize the loss and maximize the accuracy in training dataset, as the blue lines shown in the figures above. On the other hand, in the testing dataset, we can see that the accuracy quickly goes up in the first few epoch, which is a sign of underfitting. After the first few epoch, although the model performs better and better in the trainging dataset, the accuracy goes down in the testing dataset, which is a sign of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "If you don't have a GPU (level is higher than GTX 1060) or you are not good at setting lots of things about computer, we recommend you to use the [kaggle kernel](https://www.kaggle.com/kernels) to do deep learning model training. They have already installed all the librarys and provided free GPU for you to use.\n",
    "\n",
    "Note however that you will only be able to run a kernel for 6 hours. After 6 hours of inactivity, your Kaggle kernel will shut down (meaning if your model takes more than 6 hours to train, you can't train it at once).\n",
    "\n",
    "\n",
    "### More Information for your reference\n",
    "\n",
    "* Keras document: https://keras.io/\n",
    "* Keras GitHub example: https://github.com/keras-team/keras/tree/master/examples\n",
    "* CS229: Machine Learning: http://cs229.stanford.edu/syllabus.html\n",
    "* Deep Learning cheatsheet: https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-deep-learning\n",
    "* If you want to try TensorFlow or PyTorch: https://pytorch.org/tutorials/\n",
    "https://www.tensorflow.org/tutorials/quickstart/beginner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Word2Vector\n",
    "\n",
    "We will introduce how to use `gensim` to train your word2vec model and how to load a pre-trained model.\n",
    "\n",
    "https://radimrehurek.com/gensim/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Prepare training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## check library\n",
    "import gensim\n",
    "\n",
    "## ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# # if you want to see the training messages, you can use it\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "## the input type\n",
    "train_df['text_tokenized'] = train_df['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "train_df[['id', 'text', 'text_tokenized']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the training corpus\n",
    "training_corpus = train_df['text_tokenized'].values\n",
    "training_corpus[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Training our model\n",
    "\n",
    "You can try to train your own model. More details: https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the input type\n",
    "train_df['text_tokenized'] = train_df['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "train_df[['id', 'text', 'text_tokenized']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "## setting\n",
    "vector_dim = 100\n",
    "window_size = 5\n",
    "min_count = 1\n",
    "training_iter = 20\n",
    "\n",
    "## model\n",
    "word2vec_model = Word2Vec(sentences=training_corpus, \n",
    "                          size=vector_dim, window=window_size, \n",
    "                          min_count=min_count, iter=training_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](https://i.imgur.com/Fca3MCs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Generating word vector (embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the corresponding vector of a word\n",
    "word_vec = word2vec_model.wv['happy']\n",
    "word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most similar words\n",
    "word = 'happy'\n",
    "topn = 10\n",
    "word2vec_model.most_similar(word, topn=topn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Using a pre-trained w2v model\n",
    "\n",
    "Instead of training your own model ,you can use a model that has already been trained. Here, we see 2 ways of doing that:\n",
    "\n",
    "\n",
    "#### (1) Download model by yourself\n",
    "\n",
    "source: [GoogleNews-vectors-negative300](https://code.google.com/archive/p/word2vec/)\n",
    "\n",
    "more details: https://radimrehurek.com/gensim/models/keyedvectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "## Note: this model is very huge, this will take some time ...\n",
    "model_path = \"GoogleNews-vectors-negative300.bin\"\n",
    "w2v_google_model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "print('load ok')\n",
    "\n",
    "w2v_google_model.most_similar('happy', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Using gensim api\n",
    "\n",
    "Other pretrained models are available here: https://github.com/RaRe-Technologies/gensim-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "## If you see `SSL: CERTIFICATE_VERIFY_FAILED` error, use this:\n",
    "import ssl\n",
    "import urllib.request\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "glove_twitter_25_model = api.load(\"glove-twitter-25\")\n",
    "print('load ok')\n",
    "\n",
    "glove_twitter_25_model.most_similar('happy', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 king + woman - man = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run one of the most famous examples for Word2Vec and compute the similarity between these 3 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_google_model.most_similar(positive=['king', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 7 (Take home): **  \n",
    "\n",
    "Now, we have the word vectors, but our input data is a sequence of words (or say sentence). \n",
    "How can we utilize these \"word\" vectors to represent the sentence data and train our model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the word vectors now, so the other thing we need to do is to summarize the word vectors in the sentence and\n",
    "concatenate the word vectors together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Clustering: k-means\n",
    "\n",
    "Here we introduce how to use `sklearn` to do the basic **unsupervised learning** approach, k-means.    \n",
    "\n",
    "more details: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic concept\n",
    "\n",
    "![Image](https://i.imgur.com/PEdUf54.png)\n",
    "\n",
    "(img source: https://towardsdatascience.com/k-means-clustering-identifying-f-r-i-e-n-d-s-in-the-world-of-strangers-695537505d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering target\n",
    "target_list = ['happy', 'fear', 'angry', 'car', 'teacher', 'computer']\n",
    "print('target words: ', target_list)\n",
    "\n",
    "# convert to word vector\n",
    "X = [word2vec_model.wv[word] for word in target_list]\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# we have to decide how many cluster (k) we want\n",
    "k = 2\n",
    "\n",
    "# k-means model\n",
    "kmeans_model = KMeans(n_clusters=k)\n",
    "kmeans_model.fit(X)\n",
    "\n",
    "# cluster result\n",
    "cluster_result = kmeans_model.labels_\n",
    "\n",
    "# show\n",
    "for i in range(len(target_list)):\n",
    "    print('word: {} \\t cluster: {}'.format(target_list[i], cluster_result[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](pics/pic6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check cluster membership\n",
    "word = 'student'\n",
    "word_vec = word2vec_model.wv[word]\n",
    "kmeans_model.predict([word_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check cluster membership\n",
    "word = 'sad'\n",
    "word_vec = word2vec_model.wv[word]\n",
    "kmeans_model.predict([word_vec])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 9. High-dimension Visualization: t-SNE\n",
    "\n",
    "No matter if you use the Bag-of-words, tf-idf, or word2vec, it's very hard to see the embedding result, because the dimension is larger than 3.  \n",
    "\n",
    "In Lab 1, we already talked about PCA. We can use PCA to reduce the dimension of our data, then visualize it. However, if you dig deeper into the result, you'd find it is insufficient...\n",
    "\n",
    "Our aim will be to create a visualization similar to the one below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](pics/pic7.png)\n",
    "source: https://www.fabian-keller.de/research/high-dimensional-data-visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we would like to introduce another visualization method called t-SNE.  \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Prepare visualizing target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repare data lists like:\n",
    "    - happpy words\n",
    "    - angry words\n",
    "    - data words\n",
    "    - mining words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = ['happy', 'angry', 'data', 'mining']\n",
    "\n",
    "topn = 5\n",
    "happy_words = ['happy'] + [word_ for word_, sim_ in w2v_google_model.most_similar('happy', topn=topn)]\n",
    "angry_words = ['angry'] + [word_ for word_, sim_ in w2v_google_model.most_similar('angry', topn=topn)]        \n",
    "data_words = ['data'] + [word_ for word_, sim_ in w2v_google_model.most_similar('data', topn=topn)]        \n",
    "mining_words = ['mining'] + [word_ for word_, sim_ in w2v_google_model.most_similar('mining', topn=topn)]        \n",
    "\n",
    "print('happy_words: ', happy_words)\n",
    "print('angry_words: ', angry_words)\n",
    "print('data_words: ', data_words)\n",
    "print('mining_words: ', mining_words)\n",
    "\n",
    "target_words = happy_words + angry_words + data_words + mining_words\n",
    "print('\\ntarget words: ')\n",
    "print(target_words)\n",
    "\n",
    "print('\\ncolor list:')\n",
    "cn = topn + 1\n",
    "color = ['b'] * cn + ['g'] * cn + ['r'] * cn + ['y'] * cn\n",
    "print(color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Plot using t-SNE (2-dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "## w2v model\n",
    "model = w2v_google_model\n",
    "\n",
    "## prepare training word vectors\n",
    "size = 200\n",
    "target_size = len(target_words)\n",
    "all_word = list(model.vocab.keys())\n",
    "word_train = target_words + all_word[:size]\n",
    "X_train = model[word_train]\n",
    "\n",
    "## t-SNE model\n",
    "tsne = TSNE(n_components=2, metric='cosine', random_state=28)\n",
    "\n",
    "## training\n",
    "X_tsne = tsne.fit_transform(X_train)\n",
    "\n",
    "## plot the result\n",
    "plt.figure(figsize=(7.5, 7.5), dpi=115)\n",
    "plt.scatter(X_tsne[:target_size, 0], X_tsne[:target_size, 1], c=color)\n",
    "for label, x, y in zip(target_words, X_tsne[:target_size, 0], X_tsne[:target_size, 1]):\n",
    "    plt.annotate(label, xy=(x,y), xytext=(0,0),  textcoords='offset points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 8 (Take home): **  \n",
    "\n",
    "Generate a t-SNE visualization to show the 15 words most related to the words \"angry\", \"happy\", \"sad\", \"fear\" (60 words total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "word_list1 = ['angry', 'happy', 'sad', 'fear']\n",
    "\n",
    "topn = 14\n",
    "angry_words1 = ['angry'] + [word_ for word_, sim_ in w2v_google_model.most_similar('angry', topn=topn)]\n",
    "happy_words1 = ['happy'] + [word_ for word_, sim_ in w2v_google_model.most_similar('happy', topn=topn)]        \n",
    "sad_words1 = ['sad'] + [word_ for word_, sim_ in w2v_google_model.most_similar('sad', topn=topn)]        \n",
    "fear_words1 = ['fear'] + [word_ for word_, sim_ in w2v_google_model.most_similar('fear', topn=topn)]   \n",
    "\n",
    "print('angry_words: ', angry_words1)\n",
    "print('happy_words: ', happy_words1)\n",
    "print('sad_words: ', sad_words1)\n",
    "print('fear_words: ', fear_words1)\n",
    "\n",
    "target_words1 = angry_words1 + happy_words1 + sad_words1 + fear_words1\n",
    "print('\\ntarget words: ')\n",
    "print(target_words1)\n",
    "print(len(target_words1))\n",
    "\n",
    "print('\\ncolor list:')\n",
    "cn = topn + 1\n",
    "color = ['b'] * cn + ['g'] * cn + ['r'] * cn + ['y'] * cn\n",
    "print(color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "## w2v model\n",
    "model = w2v_google_model\n",
    "\n",
    "## prepare training word vectors\n",
    "size = 1000\n",
    "target_size1 = len(target_words1)\n",
    "all_word1 = list(model.vocab.keys())\n",
    "word_train1 = target_words1 + all_word1[:size]\n",
    "X_train1 = model[word_train1]\n",
    "\n",
    "## t-SNE model\n",
    "tsne1 = TSNE(n_components=2, metric='cosine', random_state=28)\n",
    "\n",
    "## training\n",
    "X_tsne1 = tsne1.fit_transform(X_train1)\n",
    "\n",
    "## plot the result\n",
    "plt.figure(figsize=(7.5, 7.5), dpi=115)\n",
    "plt.scatter(X_tsne1[:target_size1, 0], X_tsne1[:target_size1, 1], c=color)\n",
    "for label, x, y in zip(target_words1, X_tsne1[:target_size1, 0], X_tsne1[:target_size1, 1]):\n",
    "    plt.annotate(label, xy=(x,y), xytext=(0,0),  textcoords='offset points')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 594.85,
   "position": {
    "height": "40px",
    "left": "723px",
    "right": "20px",
    "top": "80px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
